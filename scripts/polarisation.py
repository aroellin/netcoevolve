#!/usr/bin/env python3
"""
Polarisation summary generator

Reads simulation CSV files (generated by src/stats.rs), detects if the
process has stabilised in terms of `col0` and `col1` over the last K rows,
and records the polarisation time (the first time when the final stable
values start) and the level (min(col0, col1)).

Usage:
  python scripts/polarisation.py --dir output --k 10 --out polarisation-summary.csv

Defaults:
  --dir defaults to 'output'
  --k   defaults to 10
  --out defaults to 'polarisation-summary.csv' in the current working dir
"""
from __future__ import annotations

import argparse
import os
import glob
import sys
from typing import Dict, Any, Tuple, Optional, List

import pandas as pd


PARAM_KEYS_FLOAT = [
  "rho", "eta", "sd0", "sd1", "sc0", "sc1",
  "p1", "p00", "p01", "p11", "sample_delta", "t_max",
]
PARAM_KEYS_INT = ["n", "seed"]
PARAM_KEYS_ALL = PARAM_KEYS_INT + PARAM_KEYS_FLOAT


def first_nonempty_line(path: str) -> str:
  """Return the first non-empty line (stripped) or empty string if none."""
  try:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
      for _ in range(20):
        line = f.readline()
        if not line:
          break
        s = line.lstrip("\ufeff").strip()
        if s:
          return s
  except Exception:
    return ""
  return ""

def check_simulation_csv(path: str) -> Tuple[bool, str]:
  """Check whether the CSV appears to be a simulation result.

  Criteria: first non-empty line must start with '# netcoevolve=' or '# n='.
  Returns (ok, reason). If not ok, reason explains why it was skipped.
  """
  s = first_nonempty_line(path)
  if not s:
    return False, "file is empty or has no non-empty lines"
  if s.startswith("# netcoevolve=") or s.startswith("# n="):
    return True, ""
  return False, f"first non-empty line does not start with '# netcoevolve=' or '# n=' (got: {s[:80]!r})"


def parse_header_params(path: str) -> Dict[str, Any]:
  """Parse the first parameter header line from a simulation CSV file.

  The header written by stats.rs looks like:
  # netcoevolve=0.1.0 n=500 rho=500 eta=5 sd0=0 sd1=1 sc0=1 sc1=0 p1=0.5 p00=0.2 p01=0.8 p11=0.2 sample_delta=0.002 t_max=2 seed=15414 (random) output_file=output/...

  Returns a dict with keys from PARAM_KEYS_ALL (others ignored). Types cast to
  int/float accordingly when possible; missing are set to None.
  """
  params: Dict[str, Any] = {k: None for k in PARAM_KEYS_ALL}
  with open(path, "r", encoding="utf-8", errors="ignore") as f:
    # collect up to 10 header lines starting with '#'
    for _ in range(10):
      line = f.readline()
      if not line:
        break
      s = line.lstrip("\ufeff").lstrip()
      if not s:
        continue
      if not s.startswith("#"):
        break
      # drop leading '#'
      content = s[1:].strip()
      # tokenise on whitespace; parse key=value tokens
      for tok in content.split():
        if "=" not in tok:
          continue
        key, val = tok.split("=", 1)
        key = key.strip()
        val = val.strip()
        if key in PARAM_KEYS_INT:
          try:
            params[key] = int(val)
          except Exception:
            try:
              params[key] = int(float(val))
            except Exception:
              params[key] = None
        elif key in PARAM_KEYS_FLOAT:
          try:
            params[key] = float(val)
          except Exception:
            params[key] = None
        else:
          # ignore other keys
          pass
  return params


def detect_polarisation(df: pd.DataFrame, k: int) -> Tuple[bool, Optional[float], Optional[float]]:
  """Detect stabilisation over the last k rows for columns col0 and col1.

  Returns (polarised, level, time):
    - polarised: True if both col0 and col1 have constant values over the last k rows
    - level: the smaller of the two final values (min(col0_last, col1_last)) or None
    - time: the earliest time from which the final values remain constant, or None
  """
  if df is None or df.empty:
    return False, None, None
  if "col0" not in df.columns or "col1" not in df.columns or "time" not in df.columns:
    return False, None, None
  if len(df) < k:
    return False, None, None
  col0_tail = df["col0"].tail(k)
  col1_tail = df["col1"].tail(k)
  if col0_tail.nunique(dropna=False) != 1 or col1_tail.nunique(dropna=False) != 1:
    return False, None, None
  # Final values
  v0 = float(df["col0"].iloc[-1])
  v1 = float(df["col1"].iloc[-1])
  # Walk backwards to find the first index where (col0, col1) equals (v0, v1)
  i = len(df) - 1
  while i >= 0:
    if float(df["col0"].iloc[i]) == v0 and float(df["col1"].iloc[i]) == v1:
      i -= 1
    else:
      break
  start_idx = i + 1
  # If we reached the beginning, start_idx could be 0
  pol_time = float(df["time"].iloc[start_idx]) if start_idx < len(df) else None
  level = min(v0, v1)
  return True, level, pol_time


def process_file(path: str, k: int) -> Dict[str, Any]:
  """Process one CSV file, returning a flat dict of parameters and results."""
  params = parse_header_params(path)
  # Read as pandas, skipping '#' comment lines; first non-comment line is header
  try:
    df = pd.read_csv(path, comment="#")
  except Exception:
    df = pd.DataFrame()
  polarised, level, pol_time = detect_polarisation(df, k)
  row: Dict[str, Any] = {
    "file": os.path.basename(path),
    "path": path,
    "polarised": polarised,
    "polarisation_level": level if polarised else "N/A",
    "polarisation_time": pol_time if polarised else "N/A",
  }
  # Merge selected parameters
  for key in PARAM_KEYS_ALL:
    row[key] = params.get(key)
  return row


def main(argv: Optional[List[str]] = None) -> None:
  ap = argparse.ArgumentParser(description="Summarise polarisation across simulation CSV files.")
  ap.add_argument("--dir", default="output", help="Directory containing simulation CSV files (default: output)")
  ap.add_argument("--k", type=int, default=10, help="Window size from the end to require stability (default: 10)")
  ap.add_argument("--out", default=None, help="Output CSV path for the summary (default: DIR/polarisation.csv)")
  args = ap.parse_args(argv)
  if args.out is None:
    args.out = os.path.join(args.dir, "polarisation.csv")

  in_dir = args.dir
  pattern = os.path.join(in_dir, "*.csv")
  files = sorted(glob.glob(pattern))

  if not files:
    print(f"No CSV files found in directory: {in_dir}")
    # still write an empty file with headers for convenience
    cols = ["file", "path", "polarised", "polarisation_level", "polarisation_time"] + PARAM_KEYS_ALL
    pd.DataFrame(columns=cols).to_csv(args.out, index=False)
    return

  # Filter to simulation CSVs only; capture first skip reason
  sim_files: List[str] = []
  skipped_non_sim = 0
  first_skip_reason: Optional[str] = None
  for p in files:
    ok, reason = check_simulation_csv(p)
    if ok:
      sim_files.append(p)
    else:
      skipped_non_sim += 1
      if first_skip_reason is None:
        first_skip_reason = f"{os.path.basename(p)}: {reason}"

  if not sim_files:
    print(f"No simulation CSV files matched in: {in_dir}")
    if skipped_non_sim:
      print(f"Skipped non-simulation files: {skipped_non_sim}")
      if first_skip_reason:
        print(f"First skipped reason: {first_skip_reason}")
    # write empty output with headers
    cols = ["file", "path", "polarised", "polarisation_level", "polarisation_time"] + PARAM_KEYS_ALL
    pd.DataFrame(columns=cols).to_csv(args.out, index=False)
    return

  # Simple built-in progress bar (no external deps)
  def _print_progress(done: int, total: int, width: int = 40) -> None:
    if total <= 0:
      return
    frac = min(max(done / total, 0.0), 1.0)
    filled = int(frac * width)
    bar = "#" * filled + "-" * (width - filled)
    sys.stdout.write(f"\rProcessing [{bar}] {done}/{total}")
    sys.stdout.flush()

  rows = []
  total = len(sim_files)
  _print_progress(0, total)
  header_issues = 0
  first_header_issue: Optional[str] = None
  for i, p in enumerate(sim_files, start=1):
    row = process_file(p, args.k)
    # Detect header issues (missing critical keys)
    missing_keys = []
    for k in ("n", "eta"):
      if row.get(k) is None:
        missing_keys.append(k)
    if missing_keys:
      header_issues += 1
      if first_header_issue is None:
        first_header_issue = f"{os.path.basename(p)}: missing keys {', '.join(missing_keys)} in header"
    rows.append(row)
    _print_progress(i, total)
  sys.stdout.write("\n")
  sys.stdout.flush()
  df = pd.DataFrame(rows)
  # Order columns: metadata, results, then params
  ordered_cols = ["file", "path", "polarised", "polarisation_level", "polarisation_time"] + PARAM_KEYS_ALL
  # Include any extra columns that might have been added
  for c in df.columns:
    if c not in ordered_cols:
      ordered_cols.append(c)
  df = df[ordered_cols]
  df.to_csv(args.out, index=False)
  print(f"Wrote summary with {len(df)} rows to: {args.out}")
  # Print scan summary
  scanned = len(files)
  print(f"Scanned files: {scanned}")
  if skipped_non_sim:
    print(f"Skipped non-simulation files: {skipped_non_sim}")
    if first_skip_reason:
      print(f"First skipped reason: {first_skip_reason}")
  if header_issues:
    print(f"Header issues: {header_issues}")
    if first_header_issue:
      print(f"First header issue: {first_header_issue}")


if __name__ == "__main__":
  main()

